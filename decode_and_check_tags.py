# -*- coding: utf-8 -*-

import rust_hf_decoder
import re

TOKENIZER_DIR = r"C:\Experiments\project\tokenizer_rugpt3large"
INPUT_IDS     = r"C:\Experiments\project\corpus\AA_tagged\wiki_00_ids.txt"
OUTPUT_TEXT   = r"C:\Experiments\project\corpus\AA_tagged\wiki_00_decoded.txt"

MAX_TOKENS = 20000  # 0 = –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ ID

SPECIAL_TAGS = [
    "<|intellect|>",
    "<|emotion|>",
    "<|movement|>",
    "<|instinct|>",
    "<|negativeemotion|>"
]

def analyze_tags(text: str) -> None:
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–µ–≥–æ–≤."""
    for tag in SPECIAL_TAGS:
        matches = re.findall(re.escape(tag), text)
        if matches:
            print(f"üü¢ –¢–µ–≥ '{tag}' –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏ —Ü–µ–ª—ã–π.")
        else:
            print(f"üî¥ –¢–µ–≥ '{tag}' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–ª–∏ —Ä–∞–∑—Ä–µ–∑–∞–Ω.")

def main():
    print("–ó–∞–ø—É—Å–∫–∞—é Rust-–¥–µ–∫–æ–¥–µ—Ä...")
    print(f"–¢–æ–∫–µ–Ω–∞–π–∑–µ—Ä: {TOKENIZER_DIR}")
    print(f"–í—Ö–æ–¥ (ID): {INPUT_IDS}")
    print(f"–í—ã—Ö–æ–¥:     {OUTPUT_TEXT}")
    print(f"MAX_TOKENS: {MAX_TOKENS}\n")

    rust_hf_decoder.decode_file(
        TOKENIZER_DIR,
        INPUT_IDS,
        OUTPUT_TEXT,
        MAX_TOKENS,
    )

    # –ß–∏—Ç–∞–µ–º —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–≥–∏
    decoded_text = open(OUTPUT_TEXT, encoding="utf-8").read()
    analyze_tags(decoded_text)

    print("\n–ì–æ—Ç–æ–≤–æ.")
    print(f"–†–∞—Å—à–∏—Ñ—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç: {OUTPUT_TEXT}")

if __name__ == "__main__":
    main()